{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66z9BSEljTiN",
        "outputId": "bc62e0ad-8935-4c57-d5c3-7a0480cd83c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lRVIlboXm6uo"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['PYTHONHASHSEED'] = '0'\n",
        "import sys\n",
        "import time\n",
        "import numpy as np\n",
        "import random as rn\n",
        "import tensorflow as tf\n",
        "import h5py\n",
        "\n",
        "from keras.preprocessing import sequence\n",
        "from keras.layers import LSTM, GRU\n",
        "from keras.layers.wrappers import Bidirectional\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.initializers import glorot_normal, glorot_uniform\n",
        "from keras.regularizers import l1_l2, l1, l2\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from keras.utils import to_categorical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uCoN9gLdnC14"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def evaluation(y_true, y_pred, threshold=.5):\n",
        "    auc = roc_auc_score(y_true, y_pred)\n",
        "    y_pred = (y_pred > threshold).astype('int')\n",
        "    eval_dict = {'acc': accuracy_score,\n",
        "                 'mcc': matthews_corrcoef,\n",
        "                 'precision': precision_score,\n",
        "                 'recall': recall_score,\n",
        "                 'f1': f1_score}\n",
        "    for i in eval_dict:\n",
        "        ei = eval_dict[i](y_true, y_pred)\n",
        "        print(\"{:16}{:.3f}\".format(i + \":\", ei))\n",
        "    print(\"{:16}{:.3f}\".format(\"auc:\", auc))\n",
        "    return\n",
        "\n",
        "def def_kw(D, K, N=None):\n",
        "    return D[K] if K in D.keys() else N\n",
        "\n",
        "print_eq = lambda : print('\\n', '=' * 36, sep = '')\n",
        "\n",
        "def random_set(iseed=123):\n",
        "    np.random.seed(iseed)\n",
        "    rn.seed(iseed)\n",
        "    tf.random.set_seed(iseed)\n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from utilities import *\n",
        "from the_constants import BWTOOL, DNASHAPEINTER\n",
        "\n",
        "\n",
        "def get_scores(in_file, shape=None, scaled=False):\n",
        "    \"\"\" Get DNAshape values on single lines. \"\"\"\n",
        "    with open(in_file) as stream:\n",
        "        scores = []\n",
        "        for line in stream:\n",
        "            values = [item for item in line.rstrip().split()[7].split(',')\n",
        "                      if not_na(item)]\n",
        "            values = [eval(value) for value in values]\n",
        "            if scaled:\n",
        "                mini, maxi = DNASHAPEINTER[shape]\n",
        "                values, _, _ = scale01(values, mini, maxi)\n",
        "            scores.append(values)\n",
        "        return scores\n",
        "\n",
        "\n",
        "def combine_hits_shapes(hits, shapes, extension=0, binary_encoding=False):\n",
        "    \"\"\" Combine DNA sequence and shape features.\n",
        "    The hit scores (PSSM or TFFM) or 4-bits encoding are combined with DNAshape\n",
        "    features in vectors for classif.\n",
        "    \"\"\"\n",
        "    comb = []\n",
        "    index = -1\n",
        "    for hit in hits:\n",
        "        if hit:\n",
        "            index += 1\n",
        "            if shapes:\n",
        "                hit_shapes = []\n",
        "                for indx in xrange(len(shapes)):\n",
        "                    hit_shapes += shapes[indx][index]\n",
        "                if (not binary_encoding and\n",
        "                        (len(hit_shapes) ==\n",
        "                            len(shapes) * (len(hit) + 2 * extension)\n",
        "                        )\n",
        "                   ):\n",
        "                    comb.append([hit.score] + hit_shapes)\n",
        "                elif (binary_encoding and\n",
        "                        (len(hit_shapes) ==\n",
        "                            len(shapes) * (len(hit) / 4 + 2 * extension)\n",
        "                        )\n",
        "                     ):\n",
        "                    comb.append(hit + hit_shapes)\n",
        "            elif binary_encoding:\n",
        "                comb.append(hit)\n",
        "            else:\n",
        "                comb.append([hit.score])\n",
        "    return comb\n",
        "\n",
        "\n",
        "def get_shapes(hits, bed_file, first_shape, second_shape, extension=0,\n",
        "        scaled=False):\n",
        "    \"\"\" Retrieve DNAshape feature values for the hits. \"\"\"\n",
        "    bigwigs = first_shape + second_shape\n",
        "    import subprocess\n",
        "    import os\n",
        "    peaks_pos = get_positions_from_bed(bed_file)\n",
        "    with open(os.devnull, 'w') as devnull:\n",
        "        tmp_file = print_extended_hits(hits, peaks_pos, extension)\n",
        "        shapes = ['HelT', 'ProT', 'MGW', 'Roll']\n",
        "        hits_shapes = []\n",
        "        for indx, bigwig in enumerate(bigwigs):\n",
        "            if bigwig:\n",
        "                out_file = '{0}.{1}'.format(tmp_file, shapes[indx])\n",
        "                subprocess.call([BWTOOL, 'ex', 'bed', tmp_file, bigwig, out_file],\n",
        "                                stdout=devnull)\n",
        "                if indx < 4:\n",
        "                    hits_shapes.append(get_scores(out_file, shapes[indx], scaled))\n",
        "                else:\n",
        "                    hits_shapes.append(get_scores(out_file, shapes[indx]))\n",
        "        subprocess.call(['rm', '-f', '{0}.HelT'.format(tmp_file),\n",
        "            '{0}.MGW'.format(tmp_file), '{0}.ProT'.format(tmp_file),\n",
        "            '{0}.Roll'.format(tmp_file)])\n",
        "        return hits_shapes"
      ],
      "metadata": {
        "id": "JCC_BUCqs_7J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DIf4LK4Zn6gi"
      },
      "outputs": [],
      "source": [
        "def build_model(in_shape=(1000, 4), **kw):\n",
        "\n",
        "    ### defaults\n",
        "    n_classes = 2\n",
        "    ## Convolution\n",
        "    n_filter = 64\n",
        "    conv_d = 1\n",
        "    # 1D\n",
        "    filter_length = 23\n",
        "    pool_length = 8\n",
        "    sub1 = 1\n",
        "    # 2D\n",
        "    conv_size = iter([(4, 23)])\n",
        "    pool_size = iter([(1, 8)])\n",
        "    sub2 = (1, 1)\n",
        "    ## Recurrent\n",
        "    n_units = 64\n",
        "    ## Dropout\n",
        "    idrop = iter([.5, .3, .3])\n",
        "    ## Dense\n",
        "    iden = iter([32])\n",
        "    ## Regularization\n",
        "    lambda_l1 = .00000001\n",
        "    lambda_l2 = .0001\n",
        "    loss = 'binary_crossentropy'\n",
        "    opt = 'adam'\n",
        "    # opt = 'adadelta'\n",
        "    # opt = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "\n",
        "    n_filter = def_kw(kw, 'n_filter', n_filter)\n",
        "    filter_length = def_kw(kw, 'filter_length', filter_length)\n",
        "    pool_length = def_kw(kw, 'pool_length', pool_length)\n",
        "    loss = def_kw(kw, 'loss', loss)\n",
        "    out = def_kw(kw, 'out', None)\n",
        "    opt = def_kw(kw, 'opt', opt)\n",
        "\n",
        "    ### modeling\n",
        "    print(\"Building model...\")\n",
        "    print(\"input shape:\\t\", in_shape)\n",
        "    model = Sequential()\n",
        "    model.add(Reshape(in_shape, input_shape=in_shape))\n",
        "    if conv_d == 2:\n",
        "        model.add(Reshape((1, in_shape[0], in_shape[1])))\n",
        "        model.add(Permute((1, 3, 2)))\n",
        "        model.add(Conv2D(n_filter, next(conv_size),\n",
        "                         strides=sub2,\n",
        "                         padding='valid',\n",
        "                         activation='relu',\n",
        "                         data_format=\"channels_first\",\n",
        "                         ))\n",
        "        model.add(MaxPooling2D(pool_size=next(pool_size), padding='same',\n",
        "                               data_format=\"channels_first\",\n",
        "                               ))\n",
        "        model.add(Dropout(next(idrop)))\n",
        "    elif conv_d == 1:\n",
        "        model.add(Conv1D(n_filter,\n",
        "                         kernel_size=filter_length,\n",
        "                         strides=sub1,\n",
        "                         padding='valid',\n",
        "                         activation='relu',\n",
        "                         ))\n",
        "        model.add(MaxPooling1D(pool_size=pool_length, padding='same'))\n",
        "        model.add(Dropout(next(idrop)))\n",
        "    if n_units:\n",
        "        if conv_d==2:\n",
        "            model.add(Reshape((model.output_shape[1],\n",
        "                               model.output_shape[2] * model.output_shape[3])))\n",
        "            model.add(Permute((2, 1)))\n",
        "        model.add(Bidirectional(GRU(n_units), merge_mode='concat'))\n",
        "    else:\n",
        "        model.add(Flatten())\n",
        "    model.add(Dense(next(iden),\n",
        "                    activation=None))\n",
        "    model.add(Dropout(next(idrop)))\n",
        "    if loss == 'binary_crossentropy':\n",
        "        out = (1, 'sigmoid')\n",
        "    elif loss == 'categorical_crossentropy':\n",
        "        out = (n_classes, 'softmax')\n",
        "    elif loss == 'sparse_categorical_crossentropy':\n",
        "        out = (n_classes, 'softmax')\n",
        "    model.add(Dense(out[0], activation=out[1],\n",
        "                    kernel_regularizer=l1_l2(lambda_l1, lambda_l2)\n",
        "                    ))\n",
        "    model.compile(loss=loss,\n",
        "                  optimizer=opt,\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SL9UQ91TCgkE"
      },
      "outputs": [],
      "source": [
        " import numpy as np\n",
        " a1 = []\n",
        " b1 = []\n",
        " c1 = []\n",
        " print(a1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TAKXt49ToLRZ"
      },
      "outputs": [],
      "source": [
        "def get_callbacks(**kw):\n",
        "    checkprog = def_kw(kw, 'checkprog', None)\n",
        "    earlystop = def_kw(kw, 'earlystop', None)\n",
        "    callbacks = []\n",
        "    if checkprog:\n",
        "        checkmodel = os.path.exists('./models/')\n",
        "        checkmodel = './models/' if checkmodel else './'\n",
        "        checkmodel += 'model_check_' + checkprog\n",
        "        checkmodel += '.h5'\n",
        "        checkpoint = ModelCheckpoint(filepath=checkmodel,\n",
        "                                     monitor='val_loss',\n",
        "                                     verbose=0,\n",
        "                                     save_best_only=True)\n",
        "        callbacks.append(checkpoint)\n",
        "    if earlystop:\n",
        "        assert earlystop > 0, \n",
        "        earlystopping = EarlyStopping(monitor='val_loss',\n",
        "                                      patience=earlystop,\n",
        "                                      verbose=0)\n",
        "        callbacks.append(earlystopping)\n",
        "    return callbacks\n",
        "\n",
        "def get_model(model=None, weights=None):\n",
        "    if not model:\n",
        "        model = build_model()\n",
        "    elif isinstance(model, str):\n",
        "        if model[-3:] == '.h5':\n",
        "            model = load_model(model)\n",
        "        elif model[-5] == '.':\n",
        "            with open(model) as f:\n",
        "                model = f.read()\n",
        "            model = model_from_yaml(model)\n",
        "        else:\n",
        "            model = model_from_yaml(model)\n",
        "    elif isinstance(model, tuple):\n",
        "        model = model_from_yaml(model[0], model[1])\n",
        "    elif isinstance(model, list):\n",
        "        model = model_from_yaml(model[0], model[1])\n",
        "    model = model\n",
        "    if weights: model.load_weights(weights)\n",
        "    return model\n",
        "\n",
        "def save_model(model, savename=None, string_type='yaml'):\n",
        "    if not savename and isinstance(model, str):\n",
        "        savename = model\n",
        "    model = get_model(model)\n",
        "    model.save('model_'+savename+'.h5')\n",
        "    with open(savename+'.'+string_type, 'w') as f:\n",
        "        if string_type == 'yaml': f.write(model.to_yaml())\n",
        "        elif string_type == 'json': f.write(model.to_json())\n",
        "    model.save_weights(savename+'.h5', overwrite=True)\n",
        "    return\n",
        "\n",
        "\n",
        "def joint_data(files, saving=None):\n",
        "    fi = files[0]\n",
        "    X_, y_ = load_data_1(fi)\n",
        "    for fi in files[1:]:\n",
        "        X_i, y_i = load_data_1(fi)\n",
        "        X_ = np.concatenate((X_, X_i))\n",
        "        y_ = np.concatenate((y_, y_i))\n",
        "    if saving:\n",
        "        with h5py.File(saving, 'w') as h5f:\n",
        "            h5f.create_dataset('seqs', data=X_)\n",
        "            h5f.create_dataset('labels', data=y_)\n",
        "        print(\"Saved in\", saving)\n",
        "    print(\"X_ & y_:\", X_.shape, y_.shape)\n",
        "    return (X_, y_)\n",
        "\n",
        "    _ = time.time()\n",
        "    results = model.predict(X_train, batch_size=batch_size)\n",
        "    pt = time.time() - _\n",
        "    print(\"predicting time: {0:.2f}s\".format(pt))\n",
        "    results = results[:, 0]\n",
        "    evaluation(y_train, results)\n",
        "\n",
        "def validation_1(datafile=None,\n",
        "                 dataset=None,\n",
        "                 model=None,\n",
        "                 batch_size=32):\n",
        "    model = get_model(model)\n",
        "    if dataset:\n",
        "        X, y = dataset\n",
        "    if datafile:\n",
        "        X, y = load_data_1(datafile)\n",
        "    _ = time.time()\n",
        "    results = model.predict(X, batch_size=batch_size)\n",
        "    pt = time.time() - _\n",
        "    print(\"predicting time: {0:.2f}s\".format(pt))\n",
        "    results = results[:, 0]\n",
        "    evaluation(y, results)\n",
        "    return (y, results)\n",
        "\n",
        "def load_data_3(datafile):\n",
        "    print(\"Loading data \", datafile)\n",
        "    with h5py.File(datafile, 'r') as h5f:\n",
        "        X_train = h5f['train_in'].value\n",
        "        y_train = h5f['train_out'].value\n",
        "        X_valid = h5f['valid_in'].value\n",
        "        y_valid = h5f['valid_out'].value\n",
        "        X_test = h5f['test_in'].value\n",
        "        y_test = h5f['test_out'].value\n",
        "    y_train = y_train.reshape(-1)\n",
        "    y_valid = y_valid.reshape(-1)\n",
        "    y_test = y_test.reshape(-1)\n",
        "    return (X_train, y_train,\n",
        "            X_valid, y_valid,\n",
        "            X_test, y_test)\n",
        "\n",
        "def validation_3(datafile, model=None,\n",
        "                 divided=False, fit=False,\n",
        "                 earlystop=8,\n",
        "                 checkprog=None,\n",
        "                 batch_size=32,\n",
        "                 n_epoch=1):\n",
        "    model = get_model(model)\n",
        "    (X_train, y_train,\n",
        "     X_valid, y_valid,\n",
        "     X_test, y_test) = load_data_3(datafile=datafile)\n",
        "    if fit:\n",
        "        print_eq()\n",
        "        print(\"training\")\n",
        "        _ = time.time()\n",
        "        callbacks = get_callbacks(checkprog=checkprog,\n",
        "                                  earlystop=earlystop)\n",
        "        if len(X_valid) > 0 or X_valid.shape != (1, 1):\n",
        "            model.fit(X_train, y_train,\n",
        "                      batch_size=batch_size, epochs=n_epoch,\n",
        "                      validation_data=(X_valid, y_valid),\n",
        "                      callbacks=callbacks, verbose=0)\n",
        "        else:\n",
        "            model.fit(X_train, y_train,\n",
        "                      batch_size=batch_size, epochs=n_epoch,\n",
        "                      callbacks=callbacks, verbose=0)\n",
        "        pt = time.time() - _\n",
        "        print(\"training time: {0:.2f}s\".format(pt))\n",
        "    history = model.fit(X_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=n_epoch,\n",
        "                    validation_data=(X_valid, y_valid))\n",
        "    if checkprog:\n",
        "        checkmodel = './models/model_check_' + checkprog + '.h5'\n",
        "        try:\n",
        "            model = get_model(checkmodel)\n",
        "        except:\n",
        "            pass\n",
        "    print_eq()\n",
        "    print('training set:')\n",
        "    validation_1(dataset=(X_train, y_train),\n",
        "                 model=model,\n",
        "                 batch_size=batch_size)\n",
        "    if len(X_valid) > 0 or X_valid.shape != (1,1):\n",
        "        print_eq()\n",
        "        print('valid set:')\n",
        "        validation_1(dataset=(X_valid, y_valid),\n",
        "                     model=model,\n",
        "                     batch_size=batch_size)\n",
        "    print_eq()\n",
        "    print('testing set:')\n",
        "    validation_1(dataset=(X_test, y_test),\n",
        "                 model=model,\n",
        "                 batch_size=batch_size)\n",
        "    return model\n",
        "def joint_p4653():\n",
        "    filehead = './data/enh_f5_'\n",
        "    saving = filehead + '3sets.h5'\n",
        "    fnames = [filehead + 'train_%s.h5' % (i) for i in np.r_[0:5]]\n",
        "    X_train, y_train = joint_data(fnames)\n",
        "    X_valid, y_valid = load_data_1(filehead + 'valid.h5')\n",
        "    X_test, y_test = load_data_1(filehead + 'test.h5')\n",
        "    with h5py.File(saving, 'w') as h5f:\n",
        "        h5f.create_dataset('train_in', data=X_train)\n",
        "        h5f.create_dataset('train_out', data=y_train)\n",
        "        h5f.create_dataset('valid_in', data=X_valid)\n",
        "        h5f.create_dataset('valid_out', data=y_valid)\n",
        "        h5f.create_dataset('test_in', data=X_test)\n",
        "        h5f.create_dataset('test_out', data=y_test)\n",
        "    (X_train, y_train,\n",
        "     X_valid, y_valid,\n",
        "     X_test, y_test) = load_data_3(datafile=saving)\n",
        "    pass\n",
        "def split_p4653():\n",
        "    datafile = './data/enh_f5_3sets.h5'\n",
        "    filehead = './data/enh_f5_'\n",
        "    (X_train, y_train,\n",
        "     X_test, y_test) = load_data_3(datafile=datafile)\n",
        "    batch_size = 10000\n",
        "    i = 0\n",
        "    for si in np.r_[0:X_train.shape[0]:batch_size]:\n",
        "        ei = min(si + batch_size, X_train.shape[0])\n",
        "        with h5py.File(filehead + 'train_%s.h5' % (i), 'w') as h5f:\n",
        "            h5f.create_dataset('seqs', data=X_train[si:ei, ...])\n",
        "            h5f.create_dataset('labels', data=y_train[si:ei, ...])\n",
        "        i += 1\n",
        "    with h5py.File(filehead + 'valid.h5', 'w') as h5f:\n",
        "        h5f.create_dataset('seqs', data=X_valid)\n",
        "        h5f.create_dataset('labels', data=y_valid)\n",
        "    with h5py.File(filehead + '_test.h5', 'w') as h5f:\n",
        "        h5f.create_dataset('seqs', data=X_test)\n",
        "        h5f.create_dataset('labels', data=y_test)\n",
        "    pass\n",
        "\n",
        "def gen_p4653_model():\n",
        "    model = build_model((1000, 4))\n",
        "    datafile = './data/enh_f5.h5'\n",
        "    prog = 'p4653'\n",
        "    model_saving = './models/model_' + prog + '.h5'\n",
        "    model = validation_3(datafile, model,\n",
        "                         batch_size=128,\n",
        "                         checkprog=prog,\n",
        "                         divided=True, fit=True)\n",
        "    model.save(model_saving)\n",
        "    return\n",
        "\n",
        "def validation_on_Fantom():\n",
        "    datafile = './data/enh_f5.h5'\n",
        "    model = './models/model.h5'\n",
        "    print(\"evaluating model(%s) on data(%s)\" % (model, datafile))\n",
        "    model = validation_3(datafile, model,\n",
        "                         divided=True, fit=False)\n",
        "\n",
        "def independent_vista():\n",
        "    datafile = './data/vista_human_3sets.h5'\n",
        "    model = './models/model_pretrained_0.h5'\n",
        "    model_saving = './models/model_vista.h5'\n",
        "    print(\"retraining model(%s) with data(%s)\" % (model, datafile))\n",
        "    model = validation_3(datafile, model,\n",
        "                         divided=True, fit=True,\n",
        "                         n_epoch=20)\n",
        "    model.save(model_saving)\n",
        "\n",
        "\n",
        "#b1 = a1.append(acc)\n",
        "#c1 = c1.append(n_epochs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9vzI2Qq3uxCJ"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    random_set()                # set random seed\n",
        "    validation_on_p4653()       # evaluating pretrained model on training data\n",
        "    plt.plot(model.history.history['acc'], label='accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.ylim([0.5, 1])\n",
        "    plt.legend(loc = 'lower right')\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "enhancer prediction.ipynb",
      "provenance": [],
      "mount_file_id": "1vYG-ZETJsFME_vIvUdyQQ2xEwQhHRVCI",
      "authorship_tag": "ABX9TyPIudvq0RSBqRGoQAJS8lwU"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}